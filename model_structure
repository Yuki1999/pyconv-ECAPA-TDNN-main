digraph {
	graph [size="154.65,154.65"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2733753424688 [label="
 (32, 192)" fillcolor=darkolivegreen1]
	2733557873008 [label=NativeBatchNormBackward0]
	2733557872336 -> 2733557873008
	2733557872336 [label=AddmmBackward0]
	2733557873824 -> 2733557872336
	2733750782608 [label="fc6.bias
 (192)" fillcolor=lightblue]
	2733750782608 -> 2733557873824
	2733557873824 [label=AccumulateGrad]
	2724602990304 -> 2733557872336
	2724602990304 [label=NativeBatchNormBackward0]
	2724602988864 -> 2724602990304
	2724602988864 [label=CatBackward0]
	2732959837008 -> 2724602988864
	2732959837008 [label=SumBackward1]
	2732959834416 -> 2732959837008
	2732959834416 [label=MulBackward0]
	2732959834464 -> 2732959834416
	2732959834464 [label=ReluBackward0]
	2732959835136 -> 2732959834464
	2732959835136 [label=ConvolutionBackward0]
	2732959834752 -> 2732959835136
	2732959834752 [label=CatBackward0]
	2732959837104 -> 2732959834752
	2732959837104 [label=ReluBackward0]
	2732959836912 -> 2732959837104
	2732959836912 [label=AddBackward0]
	2732959834896 -> 2732959836912
	2732959834896 [label=CudnnBatchNormBackward0]
	2732959834944 -> 2732959834896
	2732959834944 [label=CatBackward0]
	2732959835808 -> 2732959834944
	2732959835808 [label=ConvolutionBackward0]
	2732959836864 -> 2732959835808
	2732959836864 [label=CatBackward0]
	2732959835040 -> 2732959836864
	2732959835040 [label=CatBackward0]
	2732958535632 -> 2732959835040
	2732958535632 [label=CatBackward0]
	2732958532656 -> 2732958535632
	2732958532656 [label=CatBackward0]
	2732958533760 -> 2732958532656
	2732958533760 [label=CatBackward0]
	2732958532176 -> 2732958533760
	2732958532176 [label=CatBackward0]
	2732958534000 -> 2732958532176
	2732958534000 [label=CatBackward0]
	2732958535440 -> 2732958534000
	2732958535440 [label=ReluBackward0]
	2733746022528 -> 2732958535440
	2733746022528 [label=CudnnBatchNormBackward0]
	2733746020656 -> 2733746022528
	2733746020656 [label=CatBackward0]
	2733746021520 -> 2733746020656
	2733746021520 [label=ConvolutionBackward0]
	2732959834608 -> 2733746021520
	2732959834608 [label=SplitBackward0]
	2733746023920 -> 2732959834608
	2733746023920 [label=ReluBackward0]
	2733746023248 -> 2733746023920
	2733746023248 [label=CudnnBatchNormBackward0]
	2733746024112 -> 2733746023248
	2733746024112 [label=CatBackward0]
	2733746024064 -> 2733746024112
	2733746024064 [label=ConvolutionBackward0]
	2732959835232 -> 2733746024064
	2732959835232 [label=CudnnBatchNormBackward0]
	2733746022480 -> 2732959835232
	2733746022480 [label=ReluBackward0]
	2733746427744 -> 2733746022480
	2733746427744 [label=CatBackward0]
	2733746428224 -> 2733746427744
	2733746428224 [label=ConvolutionBackward0]
	2733746427456 -> 2733746428224
	2733555865136 [label="conv1.pyconv_levels.0.weight
 (256, 80, 5)" fillcolor=lightblue]
	2733555865136 -> 2733746427456
	2733746427456 [label=AccumulateGrad]
	2733746426544 -> 2733746427744
	2733746426544 [label=ConvolutionBackward0]
	2733746428944 -> 2733746426544
	2733747857184 [label="conv1.pyconv_levels.1.weight
 (256, 20, 5)" fillcolor=lightblue]
	2733747857184 -> 2733746428944
	2733746428944 [label=AccumulateGrad]
	2733746429616 -> 2732959835232
	2727435715664 [label="bn1.weight
 (512)" fillcolor=lightblue]
	2727435715664 -> 2733746429616
	2733746429616 [label=AccumulateGrad]
	2733746426496 -> 2732959835232
	2733747857024 [label="bn1.bias
 (512)" fillcolor=lightblue]
	2733747857024 -> 2733746426496
	2733746426496 [label=AccumulateGrad]
	2733746023200 -> 2733746024064
	2733747857504 [label="layer1.conv1.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733747857504 -> 2733746023200
	2733746023200 [label=AccumulateGrad]
	2733746021664 -> 2733746024112
	2733746021664 [label=ConvolutionBackward0]
	2732959835232 -> 2733746021664
	2733746021568 -> 2733746021664
	2733747857744 [label="layer1.conv1.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733747857744 -> 2733746021568
	2733746021568 [label=AccumulateGrad]
	2733746021472 -> 2733746023248
	2733747857344 [label="layer1.bn1.weight
 (512)" fillcolor=lightblue]
	2733747857344 -> 2733746021472
	2733746021472 [label=AccumulateGrad]
	2733746022912 -> 2733746023248
	2733747857824 [label="layer1.bn1.bias
 (512)" fillcolor=lightblue]
	2733747857824 -> 2733746022912
	2733746022912 [label=AccumulateGrad]
	2733746020560 -> 2733746021520
	2733747858224 [label="layer1.conv2.0.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733747858224 -> 2733746020560
	2733746020560 [label=AccumulateGrad]
	2733746022672 -> 2733746020656
	2733746022672 [label=ConvolutionBackward0]
	2732959834608 -> 2733746022672
	2733746023968 -> 2733746022672
	2733747858304 [label="layer1.conv2.0.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733747858304 -> 2733746023968
	2733746023968 [label=AccumulateGrad]
	2733746022048 -> 2733746022528
	2733747858144 [label="layer1.bn2.0.weight
 (64)" fillcolor=lightblue]
	2733747858144 -> 2733746022048
	2733746022048 [label=AccumulateGrad]
	2733746024352 -> 2733746022528
	2733747858384 [label="layer1.bn2.0.bias
 (64)" fillcolor=lightblue]
	2733747858384 -> 2733746024352
	2733746024352 [label=AccumulateGrad]
	2732958534720 -> 2732958534000
	2732958534720 [label=ReluBackward0]
	2733746022336 -> 2732958534720
	2733746022336 [label=CudnnBatchNormBackward0]
	2733746020896 -> 2733746022336
	2733746020896 [label=CatBackward0]
	2733746021616 -> 2733746020896
	2733746021616 [label=ConvolutionBackward0]
	2733746427024 -> 2733746021616
	2733746427024 [label=AddBackward0]
	2732958535440 -> 2733746427024
	2732959834608 -> 2733746427024
	2733746428752 -> 2733746021616
	2733747858784 [label="layer1.conv2.1.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733747858784 -> 2733746428752
	2733746428752 [label=AccumulateGrad]
	2733746425920 -> 2733746020896
	2733746425920 [label=ConvolutionBackward0]
	2733746427024 -> 2733746425920
	2733746429904 -> 2733746425920
	2733747858864 [label="layer1.conv2.1.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733747858864 -> 2733746429904
	2733746429904 [label=AccumulateGrad]
	2733746021376 -> 2733746022336
	2733747858704 [label="layer1.bn2.1.weight
 (64)" fillcolor=lightblue]
	2733747858704 -> 2733746021376
	2733746021376 [label=AccumulateGrad]
	2733746020752 -> 2733746022336
	2733747858944 [label="layer1.bn2.1.bias
 (64)" fillcolor=lightblue]
	2733747858944 -> 2733746020752
	2733746020752 [label=AccumulateGrad]
	2732958533328 -> 2732958532176
	2732958533328 [label=ReluBackward0]
	2733746022624 -> 2732958533328
	2733746022624 [label=CudnnBatchNormBackward0]
	2733746022384 -> 2733746022624
	2733746022384 [label=CatBackward0]
	2733746426160 -> 2733746022384
	2733746426160 [label=ConvolutionBackward0]
	2733746429472 -> 2733746426160
	2733746429472 [label=AddBackward0]
	2732958534720 -> 2733746429472
	2732959834608 -> 2733746429472
	2733746428080 -> 2733746426160
	2733747859344 [label="layer1.conv2.2.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733747859344 -> 2733746428080
	2733746428080 [label=AccumulateGrad]
	2733746426592 -> 2733746022384
	2733746426592 [label=ConvolutionBackward0]
	2733746429472 -> 2733746426592
	2733746426784 -> 2733746426592
	2733750157376 [label="layer1.conv2.2.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750157376 -> 2733746426784
	2733746426784 [label=AccumulateGrad]
	2733746429184 -> 2733746022624
	2733747859264 [label="layer1.bn2.2.weight
 (64)" fillcolor=lightblue]
	2733747859264 -> 2733746429184
	2733746429184 [label=AccumulateGrad]
	2733746428032 -> 2733746022624
	2733750157456 [label="layer1.bn2.2.bias
 (64)" fillcolor=lightblue]
	2733750157456 -> 2733746428032
	2733746428032 [label=AccumulateGrad]
	2732958533952 -> 2732958533760
	2732958533952 [label=ReluBackward0]
	2733746023632 -> 2732958533952
	2733746023632 [label=CudnnBatchNormBackward0]
	2733746427360 -> 2733746023632
	2733746427360 [label=CatBackward0]
	2733746426736 -> 2733746427360
	2733746426736 [label=ConvolutionBackward0]
	2733746428896 -> 2733746426736
	2733746428896 [label=AddBackward0]
	2732958533328 -> 2733746428896
	2732959834608 -> 2733746428896
	2733746429664 -> 2733746426736
	2733750157856 [label="layer1.conv2.3.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750157856 -> 2733746429664
	2733746429664 [label=AccumulateGrad]
	2733746428176 -> 2733746427360
	2733746428176 [label=ConvolutionBackward0]
	2733746428896 -> 2733746428176
	2733746427312 -> 2733746428176
	2733750157936 [label="layer1.conv2.3.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750157936 -> 2733746427312
	2733746427312 [label=AccumulateGrad]
	2733746428800 -> 2733746023632
	2733750157776 [label="layer1.bn2.3.weight
 (64)" fillcolor=lightblue]
	2733750157776 -> 2733746428800
	2733746428800 [label=AccumulateGrad]
	2733746427504 -> 2733746023632
	2733750158016 [label="layer1.bn2.3.bias
 (64)" fillcolor=lightblue]
	2733750158016 -> 2733746427504
	2733746427504 [label=AccumulateGrad]
	2732958535296 -> 2732958532656
	2732958535296 [label=ReluBackward0]
	2732958532032 -> 2732958535296
	2732958532032 [label=CudnnBatchNormBackward0]
	2733746428464 -> 2732958532032
	2733746428464 [label=CatBackward0]
	2724602776016 -> 2733746428464
	2724602776016 [label=ConvolutionBackward0]
	2724602775296 -> 2724602776016
	2724602775296 [label=AddBackward0]
	2732958533952 -> 2724602775296
	2732959834608 -> 2724602775296
	2724602774432 -> 2724602776016
	2733750158416 [label="layer1.conv2.4.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750158416 -> 2724602774432
	2724602774432 [label=AccumulateGrad]
	2724602777264 -> 2733746428464
	2724602777264 [label=ConvolutionBackward0]
	2724602775296 -> 2724602777264
	2724602773856 -> 2724602777264
	2733750158496 [label="layer1.conv2.4.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750158496 -> 2724602773856
	2724602773856 [label=AccumulateGrad]
	2733746426688 -> 2732958532032
	2733750158336 [label="layer1.bn2.4.weight
 (64)" fillcolor=lightblue]
	2733750158336 -> 2733746426688
	2733746426688 [label=AccumulateGrad]
	2733746429520 -> 2732958532032
	2733750158576 [label="layer1.bn2.4.bias
 (64)" fillcolor=lightblue]
	2733750158576 -> 2733746429520
	2733746429520 [label=AccumulateGrad]
	2732958534288 -> 2732958535632
	2732958534288 [label=ReluBackward0]
	2732958532944 -> 2732958534288
	2732958532944 [label=CudnnBatchNormBackward0]
	2724602777504 -> 2732958532944
	2724602777504 [label=CatBackward0]
	2724602776832 -> 2724602777504
	2724602776832 [label=ConvolutionBackward0]
	2724602774912 -> 2724602776832
	2724602774912 [label=AddBackward0]
	2732958535296 -> 2724602774912
	2732959834608 -> 2724602774912
	2724602775728 -> 2724602776832
	2733750158976 [label="layer1.conv2.5.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750158976 -> 2724602775728
	2724602775728 [label=AccumulateGrad]
	2724602777120 -> 2724602777504
	2724602777120 [label=ConvolutionBackward0]
	2724602774912 -> 2724602777120
	2724602774000 -> 2724602777120
	2733750159056 [label="layer1.conv2.5.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750159056 -> 2724602774000
	2724602774000 [label=AccumulateGrad]
	2724602774240 -> 2732958532944
	2733750158896 [label="layer1.bn2.5.weight
 (64)" fillcolor=lightblue]
	2733750158896 -> 2724602774240
	2724602774240 [label=AccumulateGrad]
	2724602775392 -> 2732958532944
	2733750159136 [label="layer1.bn2.5.bias
 (64)" fillcolor=lightblue]
	2733750159136 -> 2724602775392
	2724602775392 [label=AccumulateGrad]
	2732958533088 -> 2732959835040
	2732958533088 [label=ReluBackward0]
	2732958533376 -> 2732958533088
	2732958533376 [label=CudnnBatchNormBackward0]
	2724602775968 -> 2732958533376
	2724602775968 [label=CatBackward0]
	2724602776784 -> 2724602775968
	2724602776784 [label=ConvolutionBackward0]
	2724602774864 -> 2724602776784
	2724602774864 [label=AddBackward0]
	2732958534288 -> 2724602774864
	2732959834608 -> 2724602774864
	2724602776496 -> 2724602776784
	2733750159536 [label="layer1.conv2.6.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750159536 -> 2724602776496
	2724602776496 [label=AccumulateGrad]
	2724602775008 -> 2724602775968
	2724602775008 [label=ConvolutionBackward0]
	2724602774864 -> 2724602775008
	2724602773760 -> 2724602775008
	2733750159616 [label="layer1.conv2.6.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750159616 -> 2724602773760
	2724602773760 [label=AccumulateGrad]
	2724602776976 -> 2732958533376
	2733750159456 [label="layer1.bn2.6.weight
 (64)" fillcolor=lightblue]
	2733750159456 -> 2724602776976
	2724602776976 [label=AccumulateGrad]
	2724602774336 -> 2732958533376
	2733750159696 [label="layer1.bn2.6.bias
 (64)" fillcolor=lightblue]
	2733750159696 -> 2724602774336
	2724602774336 [label=AccumulateGrad]
	2732959834608 -> 2732959836864
	2732959836000 -> 2732959835808
	2733750160096 [label="layer1.conv3.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733750160096 -> 2732959836000
	2732959836000 [label=AccumulateGrad]
	2732959837776 -> 2732959834944
	2732959837776 [label=ConvolutionBackward0]
	2732959836864 -> 2732959837776
	2732958533808 -> 2732959837776
	2733750160176 [label="layer1.conv3.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733750160176 -> 2732958533808
	2732958533808 [label=AccumulateGrad]
	2732959836528 -> 2732959834896
	2733750160016 [label="layer1.bn3.weight
 (512)" fillcolor=lightblue]
	2733750160016 -> 2732959836528
	2732959836528 [label=AccumulateGrad]
	2732959834704 -> 2732959834896
	2733750160256 [label="layer1.bn3.bias
 (512)" fillcolor=lightblue]
	2733750160256 -> 2732959834704
	2732959834704 [label=AccumulateGrad]
	2732959835232 -> 2732959836912
	2732959835424 -> 2732959834752
	2732959835424 [label=ReluBackward0]
	2732958532848 -> 2732959835424
	2732958532848 [label=AddBackward0]
	2732959837152 -> 2732958532848
	2732959837152 [label=CudnnBatchNormBackward0]
	2732959837488 -> 2732959837152
	2732959837488 [label=CatBackward0]
	2724602775776 -> 2732959837488
	2724602775776 [label=ConvolutionBackward0]
	2724602777360 -> 2724602775776
	2724602777360 [label=CatBackward0]
	2724602774576 -> 2724602777360
	2724602774576 [label=CatBackward0]
	2724602774720 -> 2724602774576
	2724602774720 [label=CatBackward0]
	2724602777552 -> 2724602774720
	2724602777552 [label=CatBackward0]
	2733557741600 -> 2724602777552
	2733557741600 [label=CatBackward0]
	2733557741936 -> 2733557741600
	2733557741936 [label=CatBackward0]
	2733557740736 -> 2733557741936
	2733557740736 [label=CatBackward0]
	2733557741504 -> 2733557740736
	2733557741504 [label=ReluBackward0]
	2733557739824 -> 2733557741504
	2733557739824 [label=CudnnBatchNormBackward0]
	2733557739968 -> 2733557739824
	2733557739968 [label=CatBackward0]
	2733557743040 -> 2733557739968
	2733557743040 [label=ConvolutionBackward0]
	2724602774960 -> 2733557743040
	2724602774960 [label=SplitBackward0]
	2733557741312 -> 2724602774960
	2733557741312 [label=ReluBackward0]
	2733557742896 -> 2733557741312
	2733557742896 [label=CudnnBatchNormBackward0]
	2733557743232 -> 2733557742896
	2733557743232 [label=CatBackward0]
	2733557739584 -> 2733557743232
	2733557739584 [label=ConvolutionBackward0]
	2732959834320 -> 2733557739584
	2732959834320 [label=AddBackward0]
	2732959835232 -> 2732959834320
	2732959837104 -> 2732959834320
	2733557739728 -> 2733557739584
	2733750160656 [label="layer2.conv1.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733750160656 -> 2733557739728
	2733557739728 [label=AccumulateGrad]
	2733557741552 -> 2733557743232
	2733557741552 [label=ConvolutionBackward0]
	2732959834320 -> 2733557741552
	2733557743184 -> 2733557741552
	2733750160736 [label="layer2.conv1.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733750160736 -> 2733557743184
	2733557743184 [label=AccumulateGrad]
	2733557742224 -> 2733557742896
	2733750160576 [label="layer2.bn1.weight
 (512)" fillcolor=lightblue]
	2733750160576 -> 2733557742224
	2733557742224 [label=AccumulateGrad]
	2733557741456 -> 2733557742896
	2733750160816 [label="layer2.bn1.bias
 (512)" fillcolor=lightblue]
	2733750160816 -> 2733557741456
	2733557741456 [label=AccumulateGrad]
	2733557743088 -> 2733557743040
	2733750161216 [label="layer2.conv2.0.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750161216 -> 2733557743088
	2733557743088 [label=AccumulateGrad]
	2733557742128 -> 2733557739968
	2733557742128 [label=ConvolutionBackward0]
	2724602774960 -> 2733557742128
	2733557740640 -> 2733557742128
	2733750161296 [label="layer2.conv2.0.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750161296 -> 2733557740640
	2733557740640 [label=AccumulateGrad]
	2733557740016 -> 2733557739824
	2733750161136 [label="layer2.bn2.0.weight
 (64)" fillcolor=lightblue]
	2733750161136 -> 2733557740016
	2733557740016 [label=AccumulateGrad]
	2733557743280 -> 2733557739824
	2733750374464 [label="layer2.bn2.0.bias
 (64)" fillcolor=lightblue]
	2733750374464 -> 2733557743280
	2733557743280 [label=AccumulateGrad]
	2733557742800 -> 2733557740736
	2733557742800 [label=ReluBackward0]
	2733557739680 -> 2733557742800
	2733557739680 [label=CudnnBatchNormBackward0]
	2733557742752 -> 2733557739680
	2733557742752 [label=CatBackward0]
	2733557741840 -> 2733557742752
	2733557741840 [label=ConvolutionBackward0]
	2733557741984 -> 2733557741840
	2733557741984 [label=AddBackward0]
	2733557741504 -> 2733557741984
	2724602774960 -> 2733557741984
	2733557742704 -> 2733557741840
	2733750374864 [label="layer2.conv2.1.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750374864 -> 2733557742704
	2733557742704 [label=AccumulateGrad]
	2733557743136 -> 2733557742752
	2733557743136 [label=ConvolutionBackward0]
	2733557741984 -> 2733557743136
	2733557740064 -> 2733557743136
	2733750374944 [label="layer2.conv2.1.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750374944 -> 2733557740064
	2733557740064 [label=AccumulateGrad]
	2733557743568 -> 2733557739680
	2733750374784 [label="layer2.bn2.1.weight
 (64)" fillcolor=lightblue]
	2733750374784 -> 2733557743568
	2733557743568 [label=AccumulateGrad]
	2733557742272 -> 2733557739680
	2733750375024 [label="layer2.bn2.1.bias
 (64)" fillcolor=lightblue]
	2733750375024 -> 2733557742272
	2733557742272 [label=AccumulateGrad]
	2733557741168 -> 2733557741936
	2733557741168 [label=ReluBackward0]
	2733557742560 -> 2733557741168
	2733557742560 [label=CudnnBatchNormBackward0]
	2733557740592 -> 2733557742560
	2733557740592 [label=CatBackward0]
	2733557740880 -> 2733557740592
	2733557740880 [label=ConvolutionBackward0]
	2724733682880 -> 2733557740880
	2724733682880 [label=AddBackward0]
	2733557742800 -> 2724733682880
	2724602774960 -> 2724733682880
	2724733683984 -> 2733557740880
	2733750375424 [label="layer2.conv2.2.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750375424 -> 2724733683984
	2724733683984 [label=AccumulateGrad]
	2733557739872 -> 2733557740592
	2733557739872 [label=ConvolutionBackward0]
	2724733682880 -> 2733557739872
	2724733684608 -> 2733557739872
	2733750375504 [label="layer2.conv2.2.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750375504 -> 2724733684608
	2724733684608 [label=AccumulateGrad]
	2733557742032 -> 2733557742560
	2733750375344 [label="layer2.bn2.2.weight
 (64)" fillcolor=lightblue]
	2733750375344 -> 2733557742032
	2733557742032 [label=AccumulateGrad]
	2733557740784 -> 2733557742560
	2733750375584 [label="layer2.bn2.2.bias
 (64)" fillcolor=lightblue]
	2733750375584 -> 2733557740784
	2733557740784 [label=AccumulateGrad]
	2733557741264 -> 2733557741600
	2733557741264 [label=ReluBackward0]
	2733557739632 -> 2733557741264
	2733557739632 [label=CudnnBatchNormBackward0]
	2733557742464 -> 2733557739632
	2733557742464 [label=CatBackward0]
	2724733683120 -> 2733557742464
	2724733683120 [label=ConvolutionBackward0]
	2724733685424 -> 2724733683120
	2724733685424 [label=AddBackward0]
	2733557741168 -> 2724733685424
	2724602774960 -> 2724733685424
	2724733684368 -> 2724733683120
	2733750375984 [label="layer2.conv2.3.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750375984 -> 2724733684368
	2724733684368 [label=AccumulateGrad]
	2724733684416 -> 2733557742464
	2724733684416 [label=ConvolutionBackward0]
	2724733685424 -> 2724733684416
	2724733685280 -> 2724733684416
	2733750376064 [label="layer2.conv2.3.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750376064 -> 2724733685280
	2724733685280 [label=AccumulateGrad]
	2724733682448 -> 2733557739632
	2733750375904 [label="layer2.bn2.3.weight
 (64)" fillcolor=lightblue]
	2733750375904 -> 2724733682448
	2724733682448 [label=AccumulateGrad]
	2724733683936 -> 2733557739632
	2733750376144 [label="layer2.bn2.3.bias
 (64)" fillcolor=lightblue]
	2733750376144 -> 2724733683936
	2724733683936 [label=AccumulateGrad]
	2733557740832 -> 2724602777552
	2733557740832 [label=ReluBackward0]
	2733557741216 -> 2733557740832
	2733557741216 [label=CudnnBatchNormBackward0]
	2724733684512 -> 2733557741216
	2724733684512 [label=CatBackward0]
	2724733685712 -> 2724733684512
	2724733685712 [label=ConvolutionBackward0]
	2724733685088 -> 2724733685712
	2724733685088 [label=AddBackward0]
	2733557741264 -> 2724733685088
	2724602774960 -> 2724733685088
	2724733684944 -> 2724733685712
	2733750376544 [label="layer2.conv2.4.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750376544 -> 2724733684944
	2724733684944 [label=AccumulateGrad]
	2724733685232 -> 2724733684512
	2724733685232 [label=ConvolutionBackward0]
	2724733685088 -> 2724733685232
	2724733684752 -> 2724733685232
	2733750376624 [label="layer2.conv2.4.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750376624 -> 2724733684752
	2724733684752 [label=AccumulateGrad]
	2724733683360 -> 2733557741216
	2733750376464 [label="layer2.bn2.4.weight
 (64)" fillcolor=lightblue]
	2733750376464 -> 2724733683360
	2724733683360 [label=AccumulateGrad]
	2724733684176 -> 2733557741216
	2733750376704 [label="layer2.bn2.4.bias
 (64)" fillcolor=lightblue]
	2733750376704 -> 2724733684176
	2724733684176 [label=AccumulateGrad]
	2733557739920 -> 2724602774720
	2733557739920 [label=ReluBackward0]
	2733557740400 -> 2733557739920
	2733557740400 [label=CudnnBatchNormBackward0]
	2724733684272 -> 2733557740400
	2724733684272 [label=CatBackward0]
	2724733684032 -> 2724733684272
	2724733684032 [label=ConvolutionBackward0]
	2724733684464 -> 2724733684032
	2724733684464 [label=AddBackward0]
	2733557740832 -> 2724733684464
	2724602774960 -> 2724733684464
	2724733682400 -> 2724733684032
	2733750377104 [label="layer2.conv2.5.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750377104 -> 2724733682400
	2724733682400 [label=AccumulateGrad]
	2724733684128 -> 2724733684272
	2724733684128 [label=ConvolutionBackward0]
	2724733684464 -> 2724733684128
	2724733682832 -> 2724733684128
	2733750377184 [label="layer2.conv2.5.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750377184 -> 2724733682832
	2724733682832 [label=AccumulateGrad]
	2724733685472 -> 2733557740400
	2733750377024 [label="layer2.bn2.5.weight
 (64)" fillcolor=lightblue]
	2733750377024 -> 2724733685472
	2724733685472 [label=AccumulateGrad]
	2724733684656 -> 2733557740400
	2733750377264 [label="layer2.bn2.5.bias
 (64)" fillcolor=lightblue]
	2733750377264 -> 2724733684656
	2724733684656 [label=AccumulateGrad]
	2724602776112 -> 2724602774576
	2724602776112 [label=ReluBackward0]
	2733557740544 -> 2724602776112
	2733557740544 [label=CudnnBatchNormBackward0]
	2724733683408 -> 2733557740544
	2724733683408 [label=CatBackward0]
	2724733682592 -> 2724733683408
	2724733682592 [label=ConvolutionBackward0]
	2724733685376 -> 2724733682592
	2724733685376 [label=AddBackward0]
	2733557739920 -> 2724733685376
	2724602774960 -> 2724733685376
	2724733682352 -> 2724733682592
	2733750377664 [label="layer2.conv2.6.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750377664 -> 2724733682352
	2724733682352 [label=AccumulateGrad]
	2724733684080 -> 2724733683408
	2724733684080 [label=ConvolutionBackward0]
	2724733685376 -> 2724733684080
	2724733683072 -> 2724733684080
	2733750377744 [label="layer2.conv2.6.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750377744 -> 2724733683072
	2724733683072 [label=AccumulateGrad]
	2724733685136 -> 2733557740544
	2733750377584 [label="layer2.bn2.6.weight
 (64)" fillcolor=lightblue]
	2733750377584 -> 2724733685136
	2724733685136 [label=AccumulateGrad]
	2724733682496 -> 2733557740544
	2733750377824 [label="layer2.bn2.6.bias
 (64)" fillcolor=lightblue]
	2733750377824 -> 2724733682496
	2724733682496 [label=AccumulateGrad]
	2724602774960 -> 2724602777360
	2724602775344 -> 2724602775776
	2733750378224 [label="layer2.conv3.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733750378224 -> 2724602775344
	2724602775344 [label=AccumulateGrad]
	2724602777456 -> 2732959837488
	2724602777456 [label=ConvolutionBackward0]
	2724602777360 -> 2724602777456
	2733557741696 -> 2724602777456
	2733750378304 [label="layer2.conv3.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733750378304 -> 2733557741696
	2733557741696 [label=AccumulateGrad]
	2732959835520 -> 2732959837152
	2733750378144 [label="layer2.bn3.weight
 (512)" fillcolor=lightblue]
	2733750378144 -> 2732959835520
	2732959835520 [label=AccumulateGrad]
	2724602775872 -> 2732959837152
	2733750378384 [label="layer2.bn3.bias
 (512)" fillcolor=lightblue]
	2733750378384 -> 2724602775872
	2724602775872 [label=AccumulateGrad]
	2732959834320 -> 2732958532848
	2732959834224 -> 2732959834752
	2732959834224 [label=ReluBackward0]
	2732959836480 -> 2732959834224
	2732959836480 [label=AddBackward0]
	2724602773568 -> 2732959836480
	2724602773568 [label=CudnnBatchNormBackward0]
	2724602775632 -> 2724602773568
	2724602775632 [label=CatBackward0]
	2724733683552 -> 2724602775632
	2724733683552 [label=ConvolutionBackward0]
	2724733682928 -> 2724733683552
	2724733682928 [label=CatBackward0]
	2724733683024 -> 2724733682928
	2724733683024 [label=CatBackward0]
	2724733684992 -> 2724733683024
	2724733684992 [label=CatBackward0]
	2732960451312 -> 2724733684992
	2732960451312 [label=CatBackward0]
	2732960449680 -> 2732960451312
	2732960449680 [label=CatBackward0]
	2732960449920 -> 2732960449680
	2732960449920 [label=CatBackward0]
	2732960451648 -> 2732960449920
	2732960451648 [label=CatBackward0]
	2732960448720 -> 2732960451648
	2732960448720 [label=ReluBackward0]
	2732960451168 -> 2732960448720
	2732960451168 [label=CudnnBatchNormBackward0]
	2732960452464 -> 2732960451168
	2732960452464 [label=CatBackward0]
	2732960452032 -> 2732960452464
	2732960452032 [label=ConvolutionBackward0]
	2724733685184 -> 2732960452032
	2724733685184 [label=SplitBackward0]
	2732960451024 -> 2724733685184
	2732960451024 [label=ReluBackward0]
	2732960450928 -> 2732960451024
	2732960450928 [label=CudnnBatchNormBackward0]
	2732960451936 -> 2732960450928
	2732960451936 [label=CatBackward0]
	2732960450592 -> 2732960451936
	2732960450592 [label=ConvolutionBackward0]
	2724602775488 -> 2732960450592
	2724602775488 [label=AddBackward0]
	2733752484816 -> 2724602775488
	2733752484816 [label=AddBackward0]
	2732959835232 -> 2733752484816
	2732959837104 -> 2733752484816
	2732959835424 -> 2724602775488
	2733752487360 -> 2732960450592
	2733750563200 [label="layer3.conv1.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733750563200 -> 2733752487360
	2733752487360 [label=AccumulateGrad]
	2733752486496 -> 2732960451936
	2733752486496 [label=ConvolutionBackward0]
	2724602775488 -> 2733752486496
	2733752485440 -> 2733752486496
	2733750563280 [label="layer3.conv1.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733750563280 -> 2733752485440
	2733752485440 [label=AccumulateGrad]
	2732960449392 -> 2732960450928
	2733750563120 [label="layer3.bn1.weight
 (512)" fillcolor=lightblue]
	2733750563120 -> 2732960449392
	2732960449392 [label=AccumulateGrad]
	2732960451360 -> 2732960450928
	2733750563360 [label="layer3.bn1.bias
 (512)" fillcolor=lightblue]
	2733750563360 -> 2732960451360
	2732960451360 [label=AccumulateGrad]
	2732960449728 -> 2732960452032
	2733750563760 [label="layer3.conv2.0.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750563760 -> 2732960449728
	2732960449728 [label=AccumulateGrad]
	2732960451792 -> 2732960452464
	2732960451792 [label=ConvolutionBackward0]
	2724733685184 -> 2732960451792
	2732960450832 -> 2732960451792
	2733750563840 [label="layer3.conv2.0.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750563840 -> 2732960450832
	2732960450832 [label=AccumulateGrad]
	2732960449536 -> 2732960451168
	2733750563680 [label="layer3.bn2.0.weight
 (64)" fillcolor=lightblue]
	2733750563680 -> 2732960449536
	2732960449536 [label=AccumulateGrad]
	2732960449872 -> 2732960451168
	2733750563920 [label="layer3.bn2.0.bias
 (64)" fillcolor=lightblue]
	2733750563920 -> 2732960449872
	2732960449872 [label=AccumulateGrad]
	2732960449632 -> 2732960451648
	2732960449632 [label=ReluBackward0]
	2732960452368 -> 2732960449632
	2732960452368 [label=CudnnBatchNormBackward0]
	2732960448768 -> 2732960452368
	2732960448768 [label=CatBackward0]
	2733752485152 -> 2732960448768
	2733752485152 [label=ConvolutionBackward0]
	2733752484624 -> 2733752485152
	2733752484624 [label=AddBackward0]
	2732960448720 -> 2733752484624
	2724733685184 -> 2733752484624
	2733752487792 -> 2733752485152
	2733750564320 [label="layer3.conv2.1.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750564320 -> 2733752487792
	2733752487792 [label=AccumulateGrad]
	2733752485200 -> 2732960448768
	2733752485200 [label=ConvolutionBackward0]
	2733752484624 -> 2733752485200
	2733752484480 -> 2733752485200
	2733750564400 [label="layer3.conv2.1.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750564400 -> 2733752484480
	2733752484480 [label=AccumulateGrad]
	2732960448672 -> 2732960452368
	2733750564240 [label="layer3.bn2.1.weight
 (64)" fillcolor=lightblue]
	2733750564240 -> 2732960448672
	2732960448672 [label=AccumulateGrad]
	2732960449248 -> 2732960452368
	2733750564480 [label="layer3.bn2.1.bias
 (64)" fillcolor=lightblue]
	2733750564480 -> 2732960449248
	2732960449248 [label=AccumulateGrad]
	2732960450400 -> 2732960449920
	2732960450400 [label=ReluBackward0]
	2732960450544 -> 2732960450400
	2732960450544 [label=CudnnBatchNormBackward0]
	2732960448624 -> 2732960450544
	2732960448624 [label=CatBackward0]
	2733752486352 -> 2732960448624
	2733752486352 [label=ConvolutionBackward0]
	2733752485344 -> 2733752486352
	2733752485344 [label=AddBackward0]
	2732960449632 -> 2733752485344
	2724733685184 -> 2733752485344
	2733752486592 -> 2733752486352
	2733750564880 [label="layer3.conv2.2.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750564880 -> 2733752486592
	2733752486592 [label=AccumulateGrad]
	2733752485776 -> 2732960448624
	2733752485776 [label=ConvolutionBackward0]
	2733752485344 -> 2733752485776
	2733752485104 -> 2733752485776
	2733750564960 [label="layer3.conv2.2.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750564960 -> 2733752485104
	2733752485104 [label=AccumulateGrad]
	2733752484528 -> 2732960450544
	2733750564800 [label="layer3.bn2.2.weight
 (64)" fillcolor=lightblue]
	2733750564800 -> 2733752484528
	2733752484528 [label=AccumulateGrad]
	2733752487840 -> 2732960450544
	2733750565040 [label="layer3.bn2.2.bias
 (64)" fillcolor=lightblue]
	2733750565040 -> 2733752487840
	2733752487840 [label=AccumulateGrad]
	2732960452176 -> 2732960449680
	2732960452176 [label=ReluBackward0]
	2732960448912 -> 2732960452176
	2732960448912 [label=CudnnBatchNormBackward0]
	2733752484384 -> 2732960448912
	2733752484384 [label=CatBackward0]
	2733752485968 -> 2733752484384
	2733752485968 [label=ConvolutionBackward0]
	2733752484336 -> 2733752485968
	2733752484336 [label=AddBackward0]
	2732960450400 -> 2733752484336
	2724733685184 -> 2733752484336
	2733752485248 -> 2733752485968
	2733750565440 [label="layer3.conv2.3.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750565440 -> 2733752485248
	2733752485248 [label=AccumulateGrad]
	2733752486544 -> 2733752484384
	2733752486544 [label=ConvolutionBackward0]
	2733752484336 -> 2733752486544
	2733752484288 -> 2733752486544
	2733750565520 [label="layer3.conv2.3.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750565520 -> 2733752484288
	2733752484288 [label=AccumulateGrad]
	2733752484432 -> 2732960448912
	2733750565360 [label="layer3.bn2.3.weight
 (64)" fillcolor=lightblue]
	2733750565360 -> 2733752484432
	2733752484432 [label=AccumulateGrad]
	2733752486400 -> 2732960448912
	2733750565600 [label="layer3.bn2.3.bias
 (64)" fillcolor=lightblue]
	2733750565600 -> 2733752486400
	2733752486400 [label=AccumulateGrad]
	2732960452128 -> 2732960451312
	2732960452128 [label=ReluBackward0]
	2732960450208 -> 2732960452128
	2732960450208 [label=CudnnBatchNormBackward0]
	2733752486112 -> 2732960450208
	2733752486112 [label=CatBackward0]
	2733752484960 -> 2733752486112
	2733752484960 [label=ConvolutionBackward0]
	2733752486448 -> 2733752484960
	2733752486448 [label=AddBackward0]
	2732960452176 -> 2733752486448
	2724733685184 -> 2733752486448
	2733752486160 -> 2733752484960
	2733750566000 [label="layer3.conv2.4.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750566000 -> 2733752486160
	2733752486160 [label=AccumulateGrad]
	2733752487312 -> 2733752486112
	2733752487312 [label=ConvolutionBackward0]
	2733752486448 -> 2733752487312
	2733752487072 -> 2733752487312
	2733750566080 [label="layer3.conv2.4.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750566080 -> 2733752487072
	2733752487072 [label=AccumulateGrad]
	2733752484768 -> 2732960450208
	2733750565920 [label="layer3.bn2.4.weight
 (64)" fillcolor=lightblue]
	2733750565920 -> 2733752484768
	2733752484768 [label=AccumulateGrad]
	2733752486064 -> 2732960450208
	2733750566160 [label="layer3.bn2.4.bias
 (64)" fillcolor=lightblue]
	2733750566160 -> 2733752486064
	2733752486064 [label=AccumulateGrad]
	2732960449200 -> 2724733684992
	2732960449200 [label=ReluBackward0]
	2732960450784 -> 2732960449200
	2732960450784 [label=CudnnBatchNormBackward0]
	2733752485824 -> 2732960450784
	2733752485824 [label=CatBackward0]
	2733752485056 -> 2733752485824
	2733752485056 [label=ConvolutionBackward0]
	2733752484912 -> 2733752485056
	2733752484912 [label=AddBackward0]
	2732960452128 -> 2733752484912
	2724733685184 -> 2733752484912
	2733752485728 -> 2733752485056
	2733750566560 [label="layer3.conv2.5.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750566560 -> 2733752485728
	2733752485728 [label=AccumulateGrad]
	2733752485536 -> 2733752485824
	2733752485536 [label=ConvolutionBackward0]
	2733752484912 -> 2733752485536
	2733752484864 -> 2733752485536
	2733750566640 [label="layer3.conv2.5.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750566640 -> 2733752484864
	2733752484864 [label=AccumulateGrad]
	2733752485008 -> 2732960450784
	2733750566480 [label="layer3.bn2.5.weight
 (64)" fillcolor=lightblue]
	2733750566480 -> 2733752485008
	2733752485008 [label=AccumulateGrad]
	2733752486256 -> 2732960450784
	2733750566720 [label="layer3.bn2.5.bias
 (64)" fillcolor=lightblue]
	2733750566720 -> 2733752486256
	2733752486256 [label=AccumulateGrad]
	2724733684896 -> 2724733683024
	2724733684896 [label=ReluBackward0]
	2732960449776 -> 2724733684896
	2732960449776 [label=CudnnBatchNormBackward0]
	2733752485584 -> 2732960449776
	2733752485584 [label=CatBackward0]
	2733752484720 -> 2733752485584
	2733752484720 [label=ConvolutionBackward0]
	2733752485488 -> 2733752484720
	2733752485488 [label=AddBackward0]
	2732960449200 -> 2733752485488
	2724733685184 -> 2733752485488
	2733752486736 -> 2733752484720
	2733750780208 [label="layer3.conv2.6.pyconv_levels.0.weight
 (32, 64, 1)" fillcolor=lightblue]
	2733750780208 -> 2733752486736
	2733752486736 [label=AccumulateGrad]
	2733752485296 -> 2733752485584
	2733752485296 [label=ConvolutionBackward0]
	2733752485488 -> 2733752485296
	2733752487120 -> 2733752485296
	2733750780288 [label="layer3.conv2.6.pyconv_levels.1.weight
 (32, 16, 1)" fillcolor=lightblue]
	2733750780288 -> 2733752487120
	2733752487120 [label=AccumulateGrad]
	2733752485920 -> 2732960449776
	2733750780128 [label="layer3.bn2.6.weight
 (64)" fillcolor=lightblue]
	2733750780128 -> 2733752485920
	2733752485920 [label=AccumulateGrad]
	2733752487168 -> 2732960449776
	2733750780368 [label="layer3.bn2.6.bias
 (64)" fillcolor=lightblue]
	2733750780368 -> 2733752487168
	2733752487168 [label=AccumulateGrad]
	2724733685184 -> 2724733682928
	2724733684848 -> 2724733683552
	2733750780768 [label="layer3.conv3.pyconv_levels.0.weight
 (256, 512, 1)" fillcolor=lightblue]
	2733750780768 -> 2724733684848
	2724733684848 [label=AccumulateGrad]
	2724733682976 -> 2724602775632
	2724733682976 [label=ConvolutionBackward0]
	2724733682928 -> 2724733682976
	2732960450304 -> 2724733682976
	2733750780848 [label="layer3.conv3.pyconv_levels.1.weight
 (256, 128, 1)" fillcolor=lightblue]
	2733750780848 -> 2732960450304
	2732960450304 [label=AccumulateGrad]
	2724602776544 -> 2724602773568
	2733750780688 [label="layer3.bn3.weight
 (512)" fillcolor=lightblue]
	2733750780688 -> 2724602776544
	2724602776544 [label=AccumulateGrad]
	2724733683168 -> 2724602773568
	2733750780928 [label="layer3.bn3.bias
 (512)" fillcolor=lightblue]
	2733750780928 -> 2724733683168
	2724733683168 [label=AccumulateGrad]
	2724602775488 -> 2732959836480
	2732959835184 -> 2732959835136
	2733750781248 [label="layer4.weight
 (1536, 1536, 1)" fillcolor=lightblue]
	2733750781248 -> 2732959835184
	2732959835184 [label=AccumulateGrad]
	2732959835952 -> 2732959835136
	2733750781328 [label="layer4.bias
 (1536)" fillcolor=lightblue]
	2733750781328 -> 2732959835952
	2732959835952 [label=AccumulateGrad]
	2732959837680 -> 2732959834416
	2732959837680 [label=SoftmaxBackward0]
	2732959834848 -> 2732959837680
	2732959834848 [label=CatBackward0]
	2724602774528 -> 2732959834848
	2724602774528 [label=ConvolutionBackward0]
	2732960451072 -> 2724602774528
	2732960451072 [label=TanhBackward0]
	2724733685616 -> 2732960451072
	2724733685616 [label=CudnnBatchNormBackward0]
	2733752486976 -> 2724733685616
	2733752486976 [label=ReluBackward0]
	2733752487888 -> 2733752486976
	2733752487888 [label=CatBackward0]
	2733752487600 -> 2733752487888
	2733752487600 [label=ConvolutionBackward0]
	2733752487456 -> 2733752487600
	2733752487456 [label=CatBackward0]
	2732959834464 -> 2733752487456
	2733752486304 -> 2733752487456
	2733752486304 [label=RepeatBackward0]
	2733752486784 -> 2733752486304
	2733752486784 [label=MeanBackward1]
	2732959834464 -> 2733752486784
	2733752487648 -> 2733752487456
	2733752487648 [label=RepeatBackward0]
	2733752486880 -> 2733752487648
	2733752486880 [label=SqrtBackward0]
	2733752487552 -> 2733752486880
	2733752487552 [label=ClampBackward1]
	2733752485632 -> 2733752487552
	2733752485632 [label=VarBackward0]
	2732959834464 -> 2733752485632
	2733752487696 -> 2733752487600
	2733750781488 [label="attention.0.pyconv_levels.0.weight
 (128, 4608, 1)" fillcolor=lightblue]
	2733750781488 -> 2733752487696
	2733752487696 [label=AccumulateGrad]
	2733752486928 -> 2733752487888
	2733752486928 [label=ConvolutionBackward0]
	2733752487456 -> 2733752486928
	2733752485392 -> 2733752486928
	2733750781568 [label="attention.0.pyconv_levels.1.weight
 (128, 1152, 1)" fillcolor=lightblue]
	2733750781568 -> 2733752485392
	2733752485392 [label=AccumulateGrad]
	2733752484576 -> 2724733685616
	2733750781408 [label="attention.2.weight
 (256)" fillcolor=lightblue]
	2733750781408 -> 2733752484576
	2733752484576 [label=AccumulateGrad]
	2733752485680 -> 2724733685616
	2733750781648 [label="attention.2.bias
 (256)" fillcolor=lightblue]
	2733750781648 -> 2733752485680
	2733752485680 [label=AccumulateGrad]
	2724733683312 -> 2724602774528
	2733750782048 [label="attention.4.pyconv_levels.0.weight
 (768, 256, 1)" fillcolor=lightblue]
	2733750782048 -> 2724733683312
	2724733683312 [label=AccumulateGrad]
	2732959834560 -> 2732959834848
	2732959834560 [label=ConvolutionBackward0]
	2732960451072 -> 2732959834560
	2724733684224 -> 2732959834560
	2733750782128 [label="attention.4.pyconv_levels.1.weight
 (768, 64, 1)" fillcolor=lightblue]
	2733750782128 -> 2724733684224
	2724733684224 [label=AccumulateGrad]
	2732959834176 -> 2724602988864
	2732959834176 [label=SqrtBackward0]
	2732959837536 -> 2732959834176
	2732959837536 [label=ClampBackward1]
	2724733684704 -> 2732959837536
	2724733684704 [label=SubBackward0]
	2732959836672 -> 2724733684704
	2732959836672 [label=SumBackward1]
	2733752486016 -> 2732959836672
	2733752486016 [label=MulBackward0]
	2733752487504 -> 2733752486016
	2733752487504 [label=PowBackward0]
	2732959834464 -> 2733752487504
	2732959837680 -> 2733752486016
	2733752487744 -> 2724733684704
	2733752487744 [label=PowBackward0]
	2732959837008 -> 2733752487744
	2732959837248 -> 2724602990304
	2733750781888 [label="bn5.weight
 (3072)" fillcolor=lightblue]
	2733750781888 -> 2732959837248
	2732959837248 [label=AccumulateGrad]
	2732959836192 -> 2724602990304
	2733750781968 [label="bn5.bias
 (3072)" fillcolor=lightblue]
	2733750781968 -> 2732959836192
	2732959836192 [label=AccumulateGrad]
	2724602989152 -> 2733557872336
	2724602989152 [label=TBackward0]
	2732959834656 -> 2724602989152
	2733750782528 [label="fc6.weight
 (192, 3072)" fillcolor=lightblue]
	2733750782528 -> 2732959834656
	2732959834656 [label=AccumulateGrad]
	2733557874544 -> 2733557873008
	2733750782688 [label="bn6.weight
 (192)" fillcolor=lightblue]
	2733750782688 -> 2733557874544
	2733557874544 [label=AccumulateGrad]
	2733557871232 -> 2733557873008
	2733750782768 [label="bn6.bias
 (192)" fillcolor=lightblue]
	2733750782768 -> 2733557871232
	2733557871232 [label=AccumulateGrad]
	2733557873008 -> 2733753424688
}
